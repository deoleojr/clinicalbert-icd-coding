# -*- coding: utf-8 -*-
"""train_logreg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YDvcpEj99Xz7s3WJZ3HBlgC2XYhlzy-Z
"""

# -*- coding: utf-8 -*-
"""
Trains and evaluates a TF-IDF + Logistic Regression model
for multi-label ICD code classification.
"""
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import classification_report, f1_score
# Import evaluation utilities if they are created
# from src.evaluation.evaluate_model import find_optimal_threshold, generate_report_df
from collections import Counter
import joblib # For saving model/vectorizer
import os
import gc
import argparse

# --- Configuration (Example - could be loaded from a config file/dict) ---
# N_TOP_CODES = 100
# MAX_TFIDF_FEATURES = 20000
# TFIDF_NGRAM_RANGE = (1, 2)
# LR_SOLVER = 'liblinear'
# LR_MAX_ITER = 1000
# LR_C = 1.0
# LR_PENALTY = 'l2'
# LR_CLASS_WEIGHT = 'balanced'
# OPTIMIZATION_THRESHOLD_STEP = 0.05
# MODEL_OUTPUT_DIR = '../../models/' # Example output path

def train_logistic_regression(processed_data_path, config):
   """Trains, evaluates, and saves the Logistic Regression model."""
   print("\n--- Starting TF-IDF + Logistic Regression Training ---")
   print(f"Loading processed data from: {processed_data_path}")
   # Load data (handle potential errors) - assumes CSV for now
   try:
       # Use low_memory=False or specify dtypes if needed
       df = pd.read_csv(processed_data_path)
       # Convert string representation of list back to list if loaded from CSV
       from ast import literal_eval
       # Adjust column name if filtering was done during preprocessing
       label_col = 'ICD9_CODE_filtered' if 'ICD9_CODE_filtered' in df.columns else 'ICD9_CODE'
       if isinstance(df[label_col].iloc[0], str):
            print(f"Converting column '{label_col}' from string to list...")
            df[label_col] = df[label_col].apply(literal_eval)
       df['TEXT'] = df['TEXT'].fillna('').astype(str) # Ensure text is string
   except Exception as e:
       print(f"Error loading or processing data file {processed_data_path}: {e}")
       return

   # --- Filter Top N Codes (if not done during preprocessing) ---
   if 'ICD9_CODE_filtered' not in df.columns:
       print(f"\nFiltering for Top {config.get('N_TOP_CODES', 100)} actual ICD codes...")
       # (Add filtering logic here if needed)
       label_col = 'ICD9_CODE_filtered'
       if label_col not in df.columns: label_col = 'ICD9_CODE' # Fallback

   # --- Binarize Labels ---
   print("\nBinarizing labels...")
   mlb = MultiLabelBinarizer()
   y = mlb.fit_transform(df[label_col])
   num_classes = len(mlb.classes_)
   print(f"Number of classes: {num_classes}")

   # --- TF-IDF ---
   print("\nCreating TF-IDF features...")
   vectorizer = TfidfVectorizer(
       max_features=config.get('MAX_TFIDF_FEATURES', 20000),
       ngram_range=config.get('TFIDF_NGRAM_RANGE', (1, 2)),
       stop_words='english'
   )
   X = vectorizer.fit_transform(df['TEXT'])
   print(f"TF-IDF Matrix shape: {X.shape}")
   del df; gc.collect()

   # --- Train/Val/Test Split ---
   print("\nSplitting data...")
   X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
   X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.125, random_state=42)
   print(f"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}")
   del X_train_full, y_train_full, X, y; gc.collect()

   # --- Model Training ---
   print("\nTraining OneVsRest Logistic Regression model...")
   log_reg = LogisticRegression(
       solver=config.get('LR_SOLVER', 'liblinear'),
       max_iter=config.get('LR_MAX_ITER', 1000),
       C=config.get('LR_C', 1.0),
       penalty=config.get('LR_PENALTY', 'l2'),
       class_weight=config.get('LR_CLASS_WEIGHT', 'balanced'),
       random_state=42
   )
   model = OneVsRestClassifier(log_reg, n_jobs=-1)
   model.fit(X_train, y_train)
   print("Model training finished.")

   # --- Evaluation (including threshold optimization) ---
   print("\nEvaluating model...")
   y_val_pred_proba = model.predict_proba(X_val)
   y_test_pred_proba = model.predict_proba(X_test)

   # Find optimal threshold (consider moving to evaluate_model.py)
   print("Finding optimal threshold...")
   best_threshold = 0.5; best_f1 = -1.0
   thresholds = np.arange(0.1, 0.6, config.get('OPTIMIZATION_THRESHOLD_STEP', 0.05))
   for threshold in thresholds:
       f1 = f1_score(y_val, (y_val_pred_proba >= threshold).astype(int), average='micro', zero_division=0)
       if f1 > best_f1: best_f1 = f1; best_threshold = threshold
   print(f"Best threshold found: {best_threshold:.2f} (Val Micro F1: {best_f1:.4f})")

   y_test_pred_bin = (y_test_pred_proba >= best_threshold).astype(int)

   # --- Reporting ---
   print("\n--- Final Test Set Performance ---")
   print(f"Using threshold: {best_threshold:.2f}")
   micro_f1 = f1_score(y_test, y_test_pred_bin, average='micro', zero_division=0)
   macro_f1 = f1_score(y_test, y_test_pred_bin, average='macro', zero_division=0)
   samples_f1 = f1_score(y_test, y_test_pred_bin, average='samples', zero_division=0)
   print(f"Test Micro F1: {micro_f1:.4f}")
   print(f"Test Macro F1: {macro_f1:.4f}")
   print(f"Test Samples F1: {samples_f1:.4f}")
   print("\nClassification Report:")
   print(classification_report(y_test, y_test_pred_bin, target_names=mlb.classes_, zero_division=0))

   # --- Save Model & Artifacts ---
   output_dir = config.get('MODEL_OUTPUT_DIR', '../../models/')
   if not os.path.exists(output_dir): os.makedirs(output_dir)
   model_path = os.path.join(output_dir, 'logreg_ovr_model.joblib')
   vectorizer_path = os.path.join(output_dir, 'tfidf_vectorizer.joblib')
   mlb_path = os.path.join(output_dir, 'mlb.joblib')
   print(f"\nSaving artifacts to {output_dir}...")
   joblib.dump(model, model_path)
   joblib.dump(vectorizer, vectorizer_path)
   joblib.dump(mlb, mlb_path)
   print("Artifacts saved.")

if __name__ == '__main__':
    # Example usage if run as a script
   parser = argparse.ArgumentParser(description='Train TF-IDF + Logistic Regression for ICD coding.')
   parser.add_argument('--data_path', type=str, required=True, help='Path to the processed CSV/Pickle file')
   parser.add_argument('--output_dir', type=str, default='../../models/logreg', help='Directory to save model artifacts')
   # Add other arguments for hyperparameters if needed

   args = parser.parse_args()

   # Simple config dictionary for demonstration
   config = {
       'N_TOP_CODES': 100,
       'MAX_TFIDF_FEATURES': 20000,
       'TFIDF_NGRAM_RANGE': (1, 2),
       'LR_SOLVER': 'liblinear',
       'LR_MAX_ITER': 1000,
       'LR_C': 1.0,
       'LR_PENALTY': 'l2',
       'LR_CLASS_WEIGHT': 'balanced',
       'OPTIMIZATION_THRESHOLD_STEP': 0.05,
       'MODEL_OUTPUT_DIR': args.output_dir
   }

   train_logistic_regression(args.data_path, config)
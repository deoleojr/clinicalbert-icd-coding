# -*- coding: utf-8 -*-
"""train_bigru.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YDvcpEj99Xz7s3WJZ3HBlgC2XYhlzy-Z
"""

# -*- coding: utf-8 -*-
"""
Trains and evaluates a Bi-directional GRU (BiGRU) model
with learned embeddings for multi-label ICD code classification.
"""
import pandas as pd
import numpy as np
import tensorflow as tf
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import classification_report, f1_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout, Input # Added Input maybe for functional API
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
# Import evaluation utilities if they are created
# from src.evaluation.evaluate_model import find_optimal_threshold, plot_history
import joblib # For saving tokenizer/mlb
import os
import gc
import argparse
import pickle

# --- Configuration (Example) ---
# N_TOP_CODES = 100
# MAX_NUM_WORDS = 20000 # Keras Tokenizer vocab size
# MAX_SEQUENCE_LENGTH = 500 # Max length for padding
# EMBEDDING_DIM = 128
# GRU_UNITS = 64
# DENSE_UNITS = 128 # Optional intermediate dense layer
# DROPOUT_RATE = 0.2 # Optional dropout
# BATCH_SIZE = 64
# EPOCHS = 15 # Or more, relying on EarlyStopping
# LEARNING_RATE = 1e-3 # Default Adam LR often works ok here
# EARLY_STOPPING_PATIENCE = 3
# OPTIMIZATION_THRESHOLD_STEP = 0.05
# MODEL_OUTPUT_DIR = '../../models/bigru/' # Example output path

def build_bigru_model(max_words, embedding_dim, max_seq_length, gru_units, num_classes, dense_units=None, dropout_rate=0.0):
   """Builds the Keras BiGRU model."""
   print("Building BiGRU model...")
   model = Sequential()
   model.add(Input(shape=(max_seq_length,), name='input_sequence')) # Define input shape explicitly
   model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_seq_length)) # Removed redundant input_length
   # Use return_sequences=True if stacking GRU layers, False for the last one feeding Dense
   model.add(Bidirectional(GRU(gru_units, return_sequences=False)))
   if dropout_rate > 0:
       model.add(Dropout(dropout_rate))
   if dense_units:
       model.add(Dense(dense_units, activation='relu'))
       if dropout_rate > 0:
           model.add(Dropout(dropout_rate))
   model.add(Dense(num_classes, activation='sigmoid')) # Multi-label output

   print("Model built successfully.")
   return model

def train_bigru(processed_data_path, config):
   """Trains, evaluates, and saves the BiGRU model."""
   print("\n--- Starting BiGRU Training ---")
   print(f"Loading processed data from: {processed_data_path}")
   try:
       df = pd.read_csv(processed_data_path)
       from ast import literal_eval
       label_col = 'ICD9_CODE_filtered' if 'ICD9_CODE_filtered' in df.columns else 'ICD9_CODE'
       if isinstance(df[label_col].iloc[0], str):
            print(f"Converting column '{label_col}' from string to list...")
            df[label_col] = df[label_col].apply(literal_eval)
       df['TEXT'] = df['TEXT'].fillna('').astype(str)
   except Exception as e:
       print(f"Error loading or processing data file {processed_data_path}: {e}")
       return

   N_TOP_CODES = config.get('N_TOP_CODES', 100)
   MAX_NUM_WORDS = config.get('MAX_NUM_WORDS', 20000)
   MAX_SEQUENCE_LENGTH = config.get('MAX_SEQUENCE_LENGTH', 500)
   EMBEDDING_DIM = config.get('EMBEDDING_DIM', 128)
   GRU_UNITS = config.get('GRU_UNITS', 64)
   DENSE_UNITS = config.get('DENSE_UNITS', 128)
   DROPOUT_RATE = config.get('DROPOUT_RATE', 0.2)
   BATCH_SIZE = config.get('BATCH_SIZE', 64)
   EPOCHS = config.get('EPOCHS', 15)
   LEARNING_RATE = config.get('LEARNING_RATE', 1e-3)
   EARLY_STOPPING_PATIENCE = config.get('EARLY_STOPPING_PATIENCE', 3)
   OPTIMIZATION_THRESHOLD_STEP = config.get('OPTIMIZATION_THRESHOLD_STEP', 0.05)
   MODEL_OUTPUT_DIR = config.get('MODEL_OUTPUT_DIR', '../../models/bigru')

   # --- Filter Top N Codes (if not done during preprocessing) ---
   if 'ICD9_CODE_filtered' not in df.columns:
       print(f"\nFiltering for Top {N_TOP_CODES} actual ICD codes...")
       # (Add filtering logic here if needed)
       label_col = 'ICD9_CODE_filtered'
       if label_col not in df.columns: label_col = 'ICD9_CODE'

   # --- Binarize Labels ---
   print("\nBinarizing labels...")
   mlb = MultiLabelBinarizer()
   y = mlb.fit_transform(df[label_col])
   num_classes = len(mlb.classes_)
   print(f"Number of classes: {num_classes}")

   # --- Tokenize and Pad Text ---
   print("\nTokenizing and padding text sequences...")
   tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<OOV>")
   texts = df['TEXT'].tolist()
   tokenizer.fit_on_texts(texts)
   sequences = tokenizer.texts_to_sequences(texts)
   X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
   print(f"Shape of data tensor (X): {X.shape}")
   del df, texts, sequences; gc.collect() # Clean up memory

   # --- Train/Val/Test Split ---
   print("\nSplitting data...")
   # Split AFTER tokenization to keep X and y aligned
   X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
   X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.125, random_state=42)
   print(f"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}")
   del X_train_full, y_train_full, X, y; gc.collect()

   # --- Build & Compile Model ---
   print("\nBuilding & Compiling BiGRU Model...")
   tf.keras.backend.clear_session() # Clear session before building
   model = build_bigru_model(
       max_words=MAX_NUM_WORDS,
       embedding_dim=EMBEDDING_DIM,
       max_seq_length=MAX_SEQUENCE_LENGTH,
       gru_units=GRU_UNITS,
       num_classes=num_classes,
       dense_units=DENSE_UNITS,
       dropout_rate=DROPOUT_RATE
   )
   optimizer = Adam(learning_rate=LEARNING_RATE)
   model.compile(optimizer=optimizer, loss='binary_crossentropy',
                 metrics=['binary_accuracy', tf.keras.metrics.AUC(name='auc_roc', multi_label=True),
                          tf.keras.metrics.AUC(name='auc_pr', curve='PR', multi_label=True)])
   print(model.summary())

   # --- Train ---
   print("\nStarting Training...")
   early_stopping = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True, verbose=1)
   try:
       history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                           epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping], verbose=1)
       print("Training finished.")
       # Add plotting here if desired, calling plot_history(history)
   except Exception as e: print(f"Training failed: {e}"); traceback.print_exc(); exit()

   # --- Evaluate ---
   print("\nEvaluating Model...")
   y_val_pred_proba = model.predict(X_val, batch_size=BATCH_SIZE)
   y_test_pred_proba = model.predict(X_test, batch_size=BATCH_SIZE)

   # Threshold optimization...
   print("Finding optimal threshold...")
   best_threshold = 0.5; best_f1 = -1.0
   thresholds = np.arange(0.1, 0.6, OPTIMIZATION_THRESHOLD_STEP)
   for threshold in thresholds:
       f1 = f1_score(y_val, (y_val_pred_proba >= threshold).astype(int), average='micro', zero_division=0)
       if f1 > best_f1: best_f1 = f1; best_threshold = threshold
   print(f"Best threshold: {best_threshold:.2f} (Val Micro F1: {best_f1:.4f})")

   y_test_pred_bin = (y_test_pred_proba >= best_threshold).astype(int)

   # Reporting...
   print("\n--- Final Test Set Performance ---")
   print(f"Using threshold: {best_threshold:.2f}")
   micro_f1 = f1_score(y_test, y_test_pred_bin, average='micro', zero_division=0)
   macro_f1 = f1_score(y_test, y_test_pred_bin, average='macro', zero_division=0)
   samples_f1 = f1_score(y_test, y_test_pred_bin, average='samples', zero_division=0)
   print(f"Test Micro F1: {micro_f1:.4f}")
   print(f"Test Macro F1: {macro_f1:.4f}")
   print(f"Test Samples F1: {samples_f1:.4f}")
   print("\nClassification Report:")
   print(classification_report(y_test, y_test_pred_bin, target_names=mlb.classes_, zero_division=0))

   # --- Save ---
   if not os.path.exists(MODEL_OUTPUT_DIR): os.makedirs(MODEL_OUTPUT_DIR)
   model_save_path = os.path.join(MODEL_OUTPUT_DIR, 'bigru_model.keras') # Use .keras format
   tokenizer_path = os.path.join(MODEL_OUTPUT_DIR, 'tokenizer.pkl')
   mlb_path = os.path.join(MODEL_OUTPUT_DIR, 'mlb.joblib')
   print(f"\nSaving artifacts to {MODEL_OUTPUT_DIR}...")
   model.save(model_save_path)
   with open(tokenizer_path, 'wb') as handle:
       pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
   joblib.dump(mlb, mlb_path)
   print("Artifacts saved.")


if __name__ == '__main__':
    # Example usage if run as a script
   parser = argparse.ArgumentParser(description='Train BiGRU model for ICD coding.')
   parser.add_argument('--data_path', type=str, required=True, help='Path to the processed CSV/Pickle file')
   parser.add_argument('--output_dir', type=str, default='../../models/bigru', help='Directory to save model artifacts')
   # Add other arguments for hyperparameters if needed (e.g., --epochs, --batch_size, --lr)

   args = parser.parse_args()

   # Simple config dictionary for demonstration
   config = {
       'N_TOP_CODES': 100,
       'MAX_NUM_WORDS': 20000,
       'MAX_SEQUENCE_LENGTH': 500,
       'EMBEDDING_DIM': 128,
       'GRU_UNITS': 64,
       'DENSE_UNITS': 128, # Can be None
       'DROPOUT_RATE': 0.2,
       'BATCH_SIZE': 64,
       'EPOCHS': 15,
       'LEARNING_RATE': 1e-3,
       'EARLY_STOPPING_PATIENCE': 3,
       'OPTIMIZATION_THRESHOLD_STEP': 0.05,
       'MODEL_OUTPUT_DIR': args.output_dir
   }

   train_bigru(args.data_path, config)
   print("\nBiGRU script finished.")
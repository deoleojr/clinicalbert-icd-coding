# -*- coding: utf-8 -*-
"""evaluate_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YDvcpEj99Xz7s3WJZ3HBlgC2XYhlzy-Z
"""

# -*- coding: utf-8 -*-
"""
Utility functions for evaluating multi-label classification models,
optimizing thresholds, and plotting results.
"""
import numpy as np
from sklearn.metrics import classification_report, f1_score
import matplotlib.pyplot as plt
import pandas as pd

def find_optimal_threshold(y_true, y_pred_proba, step=0.05, metric='micro'):
   """
   Finds the optimal prediction threshold based on maximizing F1 score
   on validation data.

   Args:
       y_true (np.ndarray): True binary labels (samples, classes).
       y_pred_proba (np.ndarray): Predicted probabilities (samples, classes).
       step (float): Step size for iterating through thresholds.
       metric (str): Averaging method for F1 score ('micro', 'macro', 'samples').

   Returns:
       tuple: (best_threshold, best_f1_score)
   """
   print(f"\nFinding optimal threshold based on validation '{metric}' F1 score...")
   best_threshold = 0.5
   best_f1 = -1.0
   thresholds = np.arange(0.1, 0.6, step) # Adjust range if needed
   f1_scores = []

   for threshold in thresholds:
       y_pred_bin = (y_pred_proba >= threshold).astype(int)
       f1 = f1_score(y_true, y_pred_bin, average=metric, zero_division=0)
       f1_scores.append(f1)
       print(f"Threshold: {threshold:.2f}, Validation {metric.capitalize()} F1: {f1:.4f}")
       if f1 > best_f1:
           best_f1 = f1
           best_threshold = threshold

   print(f"Best threshold found: {best_threshold:.2f} (Validation {metric.capitalize()} F1: {best_f1:.4f})")

   # Plot Threshold vs F1
   try:
       plt.figure(figsize=(8, 5))
       plt.plot(thresholds, f1_scores, marker='o', linestyle='-', label=f'Validation {metric.capitalize()} F1')
       plt.axvline(best_threshold, color='r', linestyle='--', label=f'Best Threshold ({best_threshold:.2f})')
       plt.title('Threshold vs. Validation F1 Score')
       plt.xlabel('Prediction Threshold')
       plt.ylabel(f'{metric.capitalize()} F1 Score')
       plt.legend()
       plt.grid(True)
       plt.show()
   except Exception as e:
       print(f"Could not plot threshold optimization: {e}")

   return best_threshold, best_f1

def plot_training_history(history):
   """Plots loss and metric curves from Keras training history."""
   print("\nPlotting training history...")
   try:
       history_df = pd.DataFrame(history.history)
       num_epochs_trained = len(history_df)
       metrics_to_plot = [m for m in history_df.columns if not m.startswith('val_') and m != 'loss']
       num_metrics = len(metrics_to_plot)

       plt.style.use('seaborn-v0_8-darkgrid')
       # Adjust subplot layout based on number of metrics
       ncols = min(num_metrics + 1, 3) # Max 3 plots per row (loss + metrics)
       nrows = (num_metrics + 1 + ncols - 1) // ncols # Calculate needed rows
       fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 5))
       axes = axes.flatten() # Flatten axes array for easy indexing

       # Plot Loss
       axes[0].plot(history_df['loss'], label='Training Loss')
       if 'val_loss' in history_df.columns:
           axes[0].plot(history_df['val_loss'], label='Validation Loss')
       axes[0].set_title('Loss Curve')
       axes[0].set_xlabel('Epoch')
       axes[0].set_ylabel('Loss')
       axes[0].legend()
       axes[0].grid(True)

       # Plot Metrics
       for i, metric in enumerate(metrics_to_plot):
           ax_idx = i + 1
           if ax_idx < len(axes):
               axes[ax_idx].plot(history_df[metric], label=f'Training {metric}')
               val_metric = f'val_{metric}'
               if val_metric in history_df.columns:
                   axes[ax_idx].plot(history_df[val_metric], label=f'Validation {metric}')
               axes[ax_idx].set_title(f'{metric.replace("_", " ").title()} Curve')
               axes[ax_idx].set_xlabel('Epoch')
               axes[ax_idx].set_ylabel(metric.replace("_", " ").title())
               axes[ax_idx].legend()
               axes[ax_idx].grid(True)

       # Hide unused subplots
       for j in range(num_metrics + 1, len(axes)):
           axes[j].set_visible(False)

       plt.tight_layout()
       plt.show()
   except Exception as e:
       print(f"Could not plot training history: {e}")


def generate_report_df(y_true_bin, y_pred_bin, class_names, description_mapping={}):
   """Generates a pandas DataFrame from classification report with descriptions."""
   print("\nGenerating Classification Report DataFrame...")
   try:
       report_dict = classification_report(y_true_bin, y_pred_bin, target_names=class_names, output_dict=True, zero_division=0)
       report_data = {code: metrics for code, metrics in report_dict.items() if code in class_names}
       df_report = pd.DataFrame.from_dict(report_data, orient='index')
       df_report['Description'] = df_report.index.map(lambda code: description_mapping.get(str(code), '--- Description Missing ---'))
       df_report = df_report[['Description', 'precision', 'recall', 'f1-score', 'support']]
       df_report = df_report.sort_values(by='f1-score', ascending=False)
       print("Classification Report DataFrame generated.")
       return df_report
   except Exception as e:
       print(f"Could not generate report DataFrame: {e}")
       return None
# -*- coding: utf-8 -*-
"""preprocess_mimic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YDvcpEj99Xz7s3WJZ3HBlgC2XYhlzy-Z
"""

# -*- coding: utf-8 -*-
"""
Handles loading raw MIMIC-III data, cleaning discharge summaries,
processing ICD-9 codes, and generating the final preprocessed DataFrame
linking notes to code lists (e.g., merged_df).
"""
import pandas as pd
import numpy as np
import re
from collections import Counter
from tqdm import tqdm
import argparse # For command-line arguments if running standalone
import os
import gc

# Assuming configuration like paths might be handled here or imported
# MIMIC_CSV_DIR = '../../data/mimic_iii/' # Example relative path
# OUTPUT_DIR = '../../data/processed/'   # Example relative path
# N_TOP_CODES = 100

def load_mimic_data(mimic_dir):
   """Loads required MIMIC-III CSV files."""
   print(f"Loading data from: {mimic_dir}")
   try:
       admissions = pd.read_csv(os.path.join(mimic_dir, "ADMISSIONS.csv.gz"))
       diagnoses = pd.read_csv(os.path.join(mimic_dir, "DIAGNOSES_ICD.csv.gz"))
       noteevents = pd.read_csv(os.path.join(mimic_dir, "NOTEEVENTS.csv.gz"), low_memory=False)
       print("Required MIMIC-III files loaded.")
       return admissions, diagnoses, noteevents
   except FileNotFoundError as e:
       print(f"Error loading files: {e}. Ensure files exist in {mimic_dir}")
       raise
   except Exception as e:
       print(f"An unexpected error occurred during data loading: {e}")
       raise

def clean_text(text):
   """Applies text cleaning rules to discharge summaries."""
   if pd.isna(text):
       return ""
   text = str(text)
   text = text.replace('\n', ' ').replace('\r', ' ')
   text = re.sub(r'\[\*\*.*?\*\*\]', '', text) # Remove MIMIC de-identification markers
   # Add more cleaning rules as developed (lowercase, punctuation, headers etc.)
   text = text.lower()
   text = re.sub(r'[^\w\s]', '', text) # Remove punctuation
   text = re.sub(r'\s+', ' ', text).strip() # Remove extra whitespace
   return text

def process_data(mimic_dir, output_file, n_top_codes=100):
   """Main function to orchestrate data loading, cleaning, merging, and saving."""
   admissions, diagnoses, noteevents = load_mimic_data(mimic_dir)

   # 1. Process Notes (Filter Discharge Summaries, clean, unique HADM_ID)
   print("Processing NOTEEVENTS...")
   notes_df = noteevents[noteevents['CATEGORY'] == 'Discharge summary'].copy()
   notes_df = notes_df[notes_df['ISERROR'].isna() | (notes_df['ISERROR'] == 0)]
   notes_df.dropna(subset=['HADM_ID', 'TEXT'], inplace=True)
   notes_df['HADM_ID'] = notes_df['HADM_ID'].astype(int)
   notes_df = notes_df[['SUBJECT_ID', 'HADM_ID', 'TEXT']]
   notes_df = notes_df.sort_values(by=['SUBJECT_ID', 'HADM_ID'])
   notes_df = notes_df.drop_duplicates(subset=['HADM_ID'], keep='first')
   print(f"Filtered to {len(notes_df)} unique discharge summaries.")
   del noteevents; gc.collect() # Free memory

   # 2. Process Diagnoses (Clean codes)
   print("Processing DIAGNOSES_ICD...")
   diag_df = diagnoses[['HADM_ID', 'SEQ_NUM', 'ICD9_CODE']].copy()
   diag_df.dropna(subset=['HADM_ID', 'ICD9_CODE'], inplace=True)
   diag_df['HADM_ID'] = diag_df['HADM_ID'].astype(int)
   diag_df['ICD9_CODE'] = diag_df['ICD9_CODE'].str.strip()
   diag_df = diag_df[diag_df['ICD9_CODE'] != '']
   del diagnoses; gc.collect() # Free memory

   # 3. Aggregate Diagnoses per Admission
   print("Aggregating diagnoses...")
   diag_df = diag_df.sort_values(by=['HADM_ID', 'SEQ_NUM'])
   diag_grouped = diag_df.groupby('HADM_ID')['ICD9_CODE'].apply(list).reset_index()
   print(f"Aggregated diagnoses for {len(diag_grouped)} admissions.")

   # 4. Merge Notes and Diagnoses
   print("Merging notes and diagnoses...")
   merged_df = pd.merge(notes_df, diag_grouped, on='HADM_ID', how='inner')
   print(f"Merged data shape: {merged_df.shape}")

   # 5. Clean Text Column
   print("Cleaning TEXT column...")
   tqdm.pandas(desc="Cleaning Text")
   merged_df['TEXT'] = merged_df['TEXT'].progress_apply(clean_text)
   merged_df = merged_df[merged_df['TEXT'].str.len() > 0] # Remove rows with empty text after cleaning
   print(f"Shape after text cleaning: {merged_df.shape}")

   # 6. Optional: Filter by Top N codes here OR do it in training script
   # If filtering here:
   # print(f"Filtering for Top {n_top_codes} codes...")
   # all_codes = [str(code) for codes_list in merged_df['ICD9_CODE'] for code in codes_list if code]
   # top_codes_counter = Counter(all_codes)
   # top_codes = [code for code, _ in top_codes_counter.most_common(n_top_codes)]
   # def filter_codes(codes_list): return [str(code) for code in codes_list if str(code) in top_codes]
   # merged_df['ICD9_CODE'] = merged_df['ICD9_CODE'].apply(filter_codes)
   # merged_df = merged_df[merged_df['ICD9_CODE'].map(len) > 0].copy()
   # print(f"Shape after filtering for top {n_top_codes} codes: {merged_df.shape}")

   # 7. Save Processed Data
   print(f"Saving processed data to: {output_file}")
   output_dir = os.path.dirname(output_file)
   if not os.path.exists(output_dir):
       os.makedirs(output_dir)
   merged_df.to_csv(output_file, index=False) # Consider pickle for faster loading/saving lists
   # merged_df.to_pickle(output_file.replace('.csv', '.pkl'))
   print("Data preprocessing finished.")

   return merged_df


if __name__ == '__main__':
   # Example usage if run as a script
   parser = argparse.ArgumentParser(description='Preprocess MIMIC-III data for ICD coding.')
   parser.add_argument('--mimic_dir', type=str, required=True, help='Directory containing MIMIC-III CSV.gz files')
   parser.add_argument('--output_file', type=str, required=True, help='Path to save the processed CSV file')
   # parser.add_argument('--top_n', type=int, default=100, help='Number of top codes to focus on (optional, can be done later)')

   args = parser.parse_args()

   process_data(args.mimic_dir, args.output_file) #, args.top_n)